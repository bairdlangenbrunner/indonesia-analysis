{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**done remotely on greenplanet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T18:21:38.594304Z",
     "start_time": "2018-07-09T18:21:35.246867Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import gdal\n",
    "import osr\n",
    "import glob\n",
    "import xarray\n",
    "import datetime\n",
    "import re\n",
    "import cartopy\n",
    "import cmocean\n",
    "import matplotlib.pyplot as mp\n",
    "import netCDF4\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open all files as a separate gdal dataset\n",
    "\n",
    "* store ALL FILES in a list called ```file_list_datetime_all``` (for MOD11A2 size~27,000(\n",
    "* store all yearday strings in a list called ```file_list_yearday_strings_all```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T18:21:50.739330Z",
     "start_time": "2018-07-09T18:21:46.310344Z"
    }
   },
   "outputs": [],
   "source": [
    "file_list_all = numpy.array(sorted(glob.glob('/beegfs/DATA/pritchard/blangenb/MODIS_ARCHIVE/MOD11A2/MOD11A2*.hdf')))\n",
    "file_list_yearday_strings_all = numpy.array([f.split('/')[-1].split('.')[1][1:] for f in file_list_all])\n",
    "file_list_datetime_all = numpy.array([datetime.datetime.strptime(yd, '%Y%j') for yd in file_list_yearday_strings_all])\n",
    "yearday_strings_unique_all = numpy.unique(file_list_yearday_strings_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify which yearday to start and end on, and subset only those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pseudocode**\n",
    "\n",
    "```\n",
    "for each year in range(2000, 2018):\n",
    "    Open up everything, gather all data, save as npy file\n",
    "    save corresponding times as npy file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = 2000\n",
    "year_end = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 2001-06-26 is missing some tiles, so I've changed the code to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for daytime LST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T18:22:00.976454Z",
     "start_time": "2018-07-09T18:22:00.944854Z"
    }
   },
   "source": [
    "variable = 'LST_day'\n",
    "instrument = 'MOD11A2'\n",
    "\n",
    "missing_indices = []\n",
    "\n",
    "for year in range(2000,2019):\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    yearday_start = str(year)+'001'\n",
    "    yearday_end = str(year+1)+'001'\n",
    "\n",
    "#     yearday_start = str(year)+'160'\n",
    "#     yearday_end = str(year)+'186'\n",
    "    \n",
    "    yearday_start_dt = datetime.datetime.strptime(yearday_start, '%Y%j')\n",
    "    yearday_end_dt = datetime.datetime.strptime(yearday_end, '%Y%j')\n",
    "    yearday_indices = [(i<yearday_end_dt)&(i>=yearday_start_dt) for i in file_list_datetime_all]\n",
    "\n",
    "    file_list = file_list_all[yearday_indices] # list of full year files\n",
    "    file_list_datetime = file_list_datetime_all[yearday_indices]\n",
    "    file_list_yearday_strings = file_list_yearday_strings_all[yearday_indices]\n",
    "    yearday_strings_unique = numpy.unique(file_list_yearday_strings)\n",
    "\n",
    "    #create the datetime list you'll iterate over in the data set\n",
    "    year_datetime_array = numpy.array(([datetime.datetime.strptime(t, '%Y%j') for t in yearday_strings_unique]))\n",
    "\n",
    "    # pull out only the tiles of interest during the given year\n",
    "    reg_exp = re.compile('.*h(27|28|29|30|31|32)v(08|09|10)(.*\\.hdf)$')\n",
    "    file_list = numpy.array([filename for filename in file_list if re.match(reg_exp, filename)])\n",
    "\n",
    "    t = 0\n",
    "    yearday_string = yearday_strings_unique[t]\n",
    "    yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "    file_list_yearday = file_list[yearday_indices]\n",
    "    \n",
    "    if file_list_yearday.__len__()!=18:\n",
    "        print(file_list_datetime[t], 'is missing')\n",
    "        missing_indices.append(t)\n",
    "        \n",
    "    else:\n",
    "        print(year_datetime_array[t])\n",
    "        \n",
    "        # open only the files for that specific \"yearday\"\n",
    "        gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "        gdal_lst_day_data = [ds.GetSubDatasets()[0][0] for ds in gdal_datasets]\n",
    "\n",
    "        # create mosaics\n",
    "        lst_day_mosaic = gdal.BuildVRT('gdal_lst_day_mosaic.vrt', gdal_lst_day_data)\n",
    "    \n",
    "        # pull out data\n",
    "        lst_day_mosaic_data = lst_day_mosaic.ReadAsArray()\n",
    "        lst_day_mosaic_ALL_DATA = numpy.zeros((year_datetime_array.size,)+lst_day_mosaic_data.shape)*numpy.nan\n",
    "        lst_day_mosaic_ALL_DATA[0,:,:] = lst_day_mosaic_data\n",
    "\n",
    "    print('========== looping over 8-day intervals ==========')\n",
    "    for t in range(1,year_datetime_array.size):\n",
    "\n",
    "        yearday_string = yearday_strings_unique[t]\n",
    "        yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "        file_list_yearday = file_list[yearday_indices]\n",
    "        \n",
    "        if file_list_yearday.__len__()!=18:\n",
    "            print(year_datetime_array[t], 'is missing')\n",
    "            missing_indices.append(t)\n",
    "        \n",
    "        else:\n",
    "            print(year_datetime_array[t])\n",
    "            # open only the files for that specific \"yearday\"\n",
    "            gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "            gdal_lst_day_data = [ds.GetSubDatasets()[0][0] for ds in gdal_datasets]\n",
    "\n",
    "            # create mosaics\n",
    "            lst_day_mosaic = gdal.BuildVRT('gdal_lst_day_mosaic.vrt', gdal_lst_day_data)\n",
    "            # pull out data\n",
    "            lst_day_mosaic_data = lst_day_mosaic.ReadAsArray()\n",
    "            # collect in storage arrays\n",
    "            lst_day_mosaic_ALL_DATA[t,:,:] = lst_day_mosaic_data\n",
    "\n",
    "    # set data outside valid range equal to nan\n",
    "    lst_day_mosaic_ALL_DATA[(lst_day_mosaic_ALL_DATA<7500)|(lst_day_mosaic_ALL_DATA>65535)] = numpy.nan\n",
    "    \n",
    "#     # scale it down to Kelvin units\n",
    "#     lst_day_mosaic_ALL_DATA_scaled = lst_day_mosaic_ALL_DATA*0.02\n",
    "#     # take the time mean\n",
    "#     lst_day_mosaic_timemean = numpy.nanmean(lst_day_mosaic_ALL_DATA_scaled, axis=0)\n",
    "    \n",
    "#     numpy.save('DATA_npy/'+instrument+'_'+variable+'_'+str(year)+'_DATA.npy', lst_day_mosaic_ALL_DATA)\n",
    "#     numpy.save('TIMES_npy/'+instrument+'_'+variable+'_'+str(year)+'_TIMES.npy', year_datetime_array)\n",
    "    \n",
    "#     dask_array = dask.array.from_array(lst_day_mosaic_ALL_DATA, chunks=(lst_day_mosaic_ALL_DATA.shape[0],100,100))\n",
    "#     dask.array.to_npy_stack('DATA_npy', dask_array)\n",
    "    \n",
    "    year_datetime_converted = netCDF4.date2num(year_datetime_array, units='days since 2000-01-01', calendar='standard')\n",
    "    lst_da = xarray.DataArray(lst_day_mosaic_ALL_DATA, dims=('time','lat','lon'))\n",
    "    lst_da['time'] = year_datetime_converted\n",
    "    lst_da['time'].attrs['units'] = 'days since 2000-01-01'\n",
    "    lst_da['time'].attrs['calendar'] = 'standard'\n",
    "    \n",
    "    lst_ds = xarray.Dataset({variable:lst_da})\n",
    "    lst_ds[variable].attrs['scale_factor'] = 0.02\n",
    "    \n",
    "    lst_ds.to_netcdf('DATA_nc/'+instrument+'_'+variable+'_'+str(year)+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for nighttime LST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variable = 'LST_night'\n",
    "instrument = 'MOD11A2'\n",
    "\n",
    "missing_indices = []\n",
    "\n",
    "for year in range(2000,2019):\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    yearday_start = str(year)+'001'\n",
    "    yearday_end = str(year+1)+'001'\n",
    "\n",
    "#     yearday_start = str(year)+'160'\n",
    "#     yearday_end = str(year)+'186'\n",
    "    \n",
    "    yearday_start_dt = datetime.datetime.strptime(yearday_start, '%Y%j')\n",
    "    yearday_end_dt = datetime.datetime.strptime(yearday_end, '%Y%j')\n",
    "    yearday_indices = [(i<yearday_end_dt)&(i>=yearday_start_dt) for i in file_list_datetime_all]\n",
    "\n",
    "    file_list = file_list_all[yearday_indices] # list of full year files\n",
    "    file_list_datetime = file_list_datetime_all[yearday_indices]\n",
    "    file_list_yearday_strings = file_list_yearday_strings_all[yearday_indices]\n",
    "    yearday_strings_unique = numpy.unique(file_list_yearday_strings)\n",
    "\n",
    "    #create the datetime list you'll iterate over in the data set\n",
    "    year_datetime_array = numpy.array(([datetime.datetime.strptime(t, '%Y%j') for t in yearday_strings_unique]))\n",
    "\n",
    "    # pull out only the tiles of interest during the given year\n",
    "    reg_exp = re.compile('.*h(27|28|29|30|31|32)v(08|09|10)(.*\\.hdf)$')\n",
    "    file_list = numpy.array([filename for filename in file_list if re.match(reg_exp, filename)])\n",
    "\n",
    "    t = 0\n",
    "    yearday_string = yearday_strings_unique[t]\n",
    "    yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "    file_list_yearday = file_list[yearday_indices]\n",
    "    \n",
    "    if file_list_yearday.__len__()!=18:\n",
    "        print(file_list_datetime[t], 'is missing')\n",
    "        missing_indices.append(t)\n",
    "        \n",
    "    else:\n",
    "        print(year_datetime_array[t])\n",
    "        \n",
    "        # open only the files for that specific \"yearday\"\n",
    "        gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "        gdal_lst_night_data = [ds.GetSubDatasets()[4][0] for ds in gdal_datasets]\n",
    "\n",
    "        # create mosaics\n",
    "        lst_night_mosaic = gdal.BuildVRT('gdal_lst_night_mosaic.vrt', gdal_lst_night_data)\n",
    "    \n",
    "        # pull out data\n",
    "        lst_night_mosaic_data = lst_night_mosaic.ReadAsArray()\n",
    "        lst_night_mosaic_ALL_DATA = numpy.zeros((year_datetime_array.size,)+lst_night_mosaic_data.shape)*numpy.nan\n",
    "        lst_night_mosaic_ALL_DATA[0,:,:] = lst_night_mosaic_data\n",
    "\n",
    "    print('========== looping over 8-day intervals ==========')\n",
    "    for t in range(1,year_datetime_array.size):\n",
    "\n",
    "        yearday_string = yearday_strings_unique[t]\n",
    "        yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "        file_list_yearday = file_list[yearday_indices]\n",
    "        \n",
    "        if file_list_yearday.__len__()!=18:\n",
    "            print(year_datetime_array[t], 'is missing')\n",
    "            missing_indices.append(t)\n",
    "        \n",
    "        else:\n",
    "            print(year_datetime_array[t])\n",
    "            # open only the files for that specific \"yearday\"\n",
    "            gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "            gdal_lst_night_data = [ds.GetSubDatasets()[4][0] for ds in gdal_datasets]\n",
    "\n",
    "            # create mosaics\n",
    "            lst_night_mosaic = gdal.BuildVRT('gdal_lst_night_mosaic.vrt', gdal_lst_night_data)\n",
    "            # pull out data\n",
    "            lst_night_mosaic_data = lst_night_mosaic.ReadAsArray()\n",
    "            # collect in storage arrays\n",
    "            lst_night_mosaic_ALL_DATA[t,:,:] = lst_night_mosaic_data\n",
    "\n",
    "    # set data outside valid range equal to nan\n",
    "    lst_night_mosaic_ALL_DATA[(lst_night_mosaic_ALL_DATA<7500)|(lst_night_mosaic_ALL_DATA>65535)] = numpy.nan\n",
    "    \n",
    "#     # scale it down to Kelvin units\n",
    "#     lst_night_mosaic_ALL_DATA_scaled = lst_night_mosaic_ALL_DATA*0.02\n",
    "#     # take the time mean\n",
    "#     lst_night_mosaic_timemean = numpy.nanmean(lst_night_mosaic_ALL_DATA_scaled, axis=0)\n",
    "    \n",
    "#     numpy.save('DATA_npy/'+instrument+'_'+variable+'_'+str(year)+'_DATA.npy', lst_night_mosaic_ALL_DATA)\n",
    "#     numpy.save('TIMES_npy/'+instrument+'_'+variable+'_'+str(year)+'_TIMES.npy', year_datetime_array)\n",
    "    \n",
    "#     dask_array = dask.array.from_array(lst_night_mosaic_ALL_DATA, chunks=(lst_night_mosaic_ALL_DATA.shape[0],100,100))\n",
    "#     dask.array.to_npy_stack('DATA_npy', dask_array)\n",
    "    \n",
    "    year_datetime_converted = netCDF4.date2num(year_datetime_array, units='days since 2000-01-01', calendar='standard')\n",
    "    lst_da = xarray.DataArray(lst_night_mosaic_ALL_DATA, dims=('time','lat','lon'))\n",
    "    lst_da['time'] = year_datetime_converted\n",
    "    lst_da['time'].attrs['units'] = 'days since 2000-01-01'\n",
    "    lst_da['time'].attrs['calendar'] = 'standard'\n",
    "    \n",
    "    lst_ds = xarray.Dataset({variable:lst_da})\n",
    "    lst_ds[variable].attrs['scale_factor'] = 0.02\n",
    "    \n",
    "    lst_ds.to_netcdf('DATA_nc/'+instrument+'_'+variable+'_'+str(year)+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for qc daytime"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variable = 'QC_day'\n",
    "instrument = 'MOD11A2'\n",
    "\n",
    "missing_indices = []\n",
    "\n",
    "for year in range(2000,2019):\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    yearday_start = str(year)+'001'\n",
    "    yearday_end = str(year+1)+'001'\n",
    "\n",
    "#     yearday_start = str(year)+'001'\n",
    "#     yearday_end = str(year)+'020'\n",
    "    \n",
    "    yearday_start_dt = datetime.datetime.strptime(yearday_start, '%Y%j')\n",
    "    yearday_end_dt = datetime.datetime.strptime(yearday_end, '%Y%j')\n",
    "    yearday_indices = [(i<yearday_end_dt)&(i>=yearday_start_dt) for i in file_list_datetime_all]\n",
    "\n",
    "    file_list = file_list_all[yearday_indices] # list of full year files\n",
    "    file_list_datetime = file_list_datetime_all[yearday_indices]\n",
    "    file_list_yearday_strings = file_list_yearday_strings_all[yearday_indices]\n",
    "    yearday_strings_unique = numpy.unique(file_list_yearday_strings)\n",
    "\n",
    "    #create the datetime list you'll iterate over in the data set\n",
    "    year_datetime_array = numpy.array(([datetime.datetime.strptime(t, '%Y%j') for t in yearday_strings_unique]))\n",
    "\n",
    "    # pull out only the tiles of interest during the given year\n",
    "    reg_exp = re.compile('.*h(27|28|29|30|31|32)v(08|09|10)(.*\\.hdf)$')\n",
    "    file_list = numpy.array([filename for filename in file_list if re.match(reg_exp, filename)])\n",
    "\n",
    "    t = 0\n",
    "    yearday_string = yearday_strings_unique[t]\n",
    "    yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "    file_list_yearday = file_list[yearday_indices]\n",
    "    \n",
    "    if file_list_yearday.__len__()!=18:\n",
    "        print(file_list_datetime[t], 'is missing')\n",
    "        missing_indices.append(t)\n",
    "        \n",
    "    else:\n",
    "        print(year_datetime_array[t])\n",
    "        \n",
    "        # open only the files for that specific \"yearday\"\n",
    "        gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "        gdal_qc_day_data = [ds.GetSubDatasets()[1][0] for ds in gdal_datasets]\n",
    "\n",
    "        # create mosaics\n",
    "        qc_day_mosaic = gdal.BuildVRT('gdal_qc_day_mosaic.vrt', gdal_qc_day_data)\n",
    "    \n",
    "        # pull out data\n",
    "        qc_day_mosaic_data = qc_day_mosaic.ReadAsArray()\n",
    "        qc_day_mosaic_ALL_DATA = numpy.zeros((year_datetime_array.size,)+qc_day_mosaic_data.shape, dtype=numpy.int)*numpy.nan\n",
    "        qc_day_mosaic_ALL_DATA[0,:,:] = qc_day_mosaic_data\n",
    "\n",
    "    print('========== looping over 8-day intervals ==========')\n",
    "    for t in range(1,year_datetime_array.size):\n",
    "\n",
    "        yearday_string = yearday_strings_unique[t]\n",
    "        yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "        file_list_yearday = file_list[yearday_indices]\n",
    "        \n",
    "        if file_list_yearday.__len__()!=18:\n",
    "            print(year_datetime_array[t], 'is missing')\n",
    "            missing_indices.append(t)\n",
    "        \n",
    "        else:\n",
    "            print(year_datetime_array[t])\n",
    "            # open only the files for that specific \"yearday\"\n",
    "            gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "            gdal_qc_day_data = [ds.GetSubDatasets()[1][0] for ds in gdal_datasets]\n",
    "\n",
    "            # create mosaics\n",
    "            qc_day_mosaic = gdal.BuildVRT('gdal_qc_day_mosaic.vrt', gdal_qc_day_data)\n",
    "            # pull out data\n",
    "            qc_day_mosaic_data = qc_day_mosaic.ReadAsArray()\n",
    "            # collect in storage arrays\n",
    "            qc_day_mosaic_ALL_DATA[t,:,:] = qc_day_mosaic_data\n",
    "\n",
    "    # set data outside valid range equal to nan\n",
    "    qc_day_mosaic_ALL_DATA[(qc_day_mosaic_ALL_DATA>255)|(qc_day_mosaic_ALL_DATA<0)] = numpy.nan\n",
    "    \n",
    "#     # scale it down to Kelvin units\n",
    "#     qc_day_mosaic_ALL_DATA_scaled = qc_day_mosaic_ALL_DATA*0.02\n",
    "#     # take the time mean\n",
    "#     qc_day_mosaic_timemean = numpy.nanmean(qc_day_mosaic_ALL_DATA_scaled, axis=0)\n",
    "    \n",
    "#     numpy.save('DATA_npy/'+instrument+'_'+variable+'_'+str(year)+'_DATA.npy', qc_day_mosaic_ALL_DATA)\n",
    "#     numpy.save('TIMES_npy/'+instrument+'_'+variable+'_'+str(year)+'_TIMES.npy', year_datetime_array)\n",
    "    \n",
    "#     dask_array = dask.array.from_array(qc_day_mosaic_ALL_DATA, chunks=(qc_day_mosaic_ALL_DATA.shape[0],100,100))\n",
    "#     dask.array.to_npy_stack('DATA_npy', dask_array)\n",
    "    \n",
    "    year_datetime_converted = netCDF4.date2num(year_datetime_array, units='days since 2000-01-01', calendar='standard')\n",
    "    qc_da = xarray.DataArray(qc_day_mosaic_ALL_DATA, dims=('time','lat','lon'))\n",
    "    qc_da['time'] = year_datetime_converted\n",
    "    qc_da['time'].attrs['units'] = 'days since 2000-01-01'\n",
    "    qc_da['time'].attrs['calendar'] = 'standard'\n",
    "    \n",
    "    qc_ds = xarray.Dataset({variable:qc_da})\n",
    "    \n",
    "    qc_ds.to_netcdf('DATA_nc/'+instrument+'_'+variable+'_'+str(year)+'.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for qc nightttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000-02-18 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2000-02-26 00:00:00\n",
      "2000-03-05 00:00:00\n",
      "2000-03-13 00:00:00\n",
      "2000-03-21 00:00:00\n",
      "2000-03-29 00:00:00\n",
      "2000-04-06 00:00:00\n",
      "2000-04-14 00:00:00\n",
      "2000-04-22 00:00:00\n",
      "2000-04-30 00:00:00\n",
      "2000-05-08 00:00:00\n",
      "2000-05-16 00:00:00\n",
      "2000-05-24 00:00:00\n",
      "2000-06-01 00:00:00\n",
      "2000-06-09 00:00:00\n",
      "2000-06-17 00:00:00\n",
      "2000-06-25 00:00:00\n",
      "2000-07-03 00:00:00\n",
      "2000-07-11 00:00:00\n",
      "2000-07-19 00:00:00\n",
      "2000-07-27 00:00:00\n",
      "2000-08-04 00:00:00\n",
      "2000-08-12 00:00:00\n",
      "2000-08-20 00:00:00\n",
      "2000-08-28 00:00:00\n",
      "2000-09-05 00:00:00\n",
      "2000-09-13 00:00:00\n",
      "2000-09-21 00:00:00\n",
      "2000-09-29 00:00:00\n",
      "2000-10-07 00:00:00\n",
      "2000-10-15 00:00:00\n",
      "2000-10-23 00:00:00\n",
      "2000-10-31 00:00:00\n",
      "2000-11-08 00:00:00\n",
      "2000-11-16 00:00:00\n",
      "2000-11-24 00:00:00\n",
      "2000-12-02 00:00:00\n",
      "2000-12-10 00:00:00\n",
      "2000-12-18 00:00:00\n",
      "2000-12-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/blangenb/miniconda3/lib/python3.6/site-packages/dask/utils.py:1010: UserWarning: Deprecated, see dask.base.get_scheduler instead\n",
      "  warnings.warn(\"Deprecated, see dask.base.get_scheduler instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "2001-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2001-01-09 00:00:00\n",
      "2001-01-17 00:00:00\n",
      "2001-01-25 00:00:00\n",
      "2001-02-02 00:00:00\n",
      "2001-02-10 00:00:00\n",
      "2001-02-18 00:00:00\n",
      "2001-02-26 00:00:00\n",
      "2001-03-06 00:00:00\n",
      "2001-03-14 00:00:00\n",
      "2001-03-22 00:00:00\n",
      "2001-03-30 00:00:00\n",
      "2001-04-07 00:00:00\n",
      "2001-04-15 00:00:00 is missing\n",
      "2001-04-23 00:00:00 is missing\n",
      "2001-05-01 00:00:00\n",
      "2001-05-09 00:00:00\n",
      "2001-05-17 00:00:00\n",
      "2001-05-25 00:00:00\n",
      "2001-06-02 00:00:00\n",
      "2001-06-10 00:00:00\n",
      "2001-06-26 00:00:00 is missing\n",
      "2001-07-04 00:00:00\n",
      "2001-07-12 00:00:00\n",
      "2001-07-20 00:00:00\n",
      "2001-07-28 00:00:00\n",
      "2001-08-05 00:00:00\n",
      "2001-08-13 00:00:00\n",
      "2001-08-21 00:00:00\n",
      "2001-08-29 00:00:00\n",
      "2001-09-06 00:00:00\n",
      "2001-09-14 00:00:00\n",
      "2001-09-22 00:00:00\n",
      "2001-09-30 00:00:00\n",
      "2001-10-08 00:00:00\n",
      "2001-10-16 00:00:00\n",
      "2001-10-24 00:00:00\n",
      "2001-11-01 00:00:00\n",
      "2001-11-09 00:00:00\n",
      "2001-11-17 00:00:00 is missing\n",
      "2001-11-25 00:00:00\n",
      "2001-12-03 00:00:00\n",
      "2001-12-11 00:00:00\n",
      "2001-12-19 00:00:00\n",
      "2001-12-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/blangenb/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in greater\n",
      "/export/home/blangenb/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in less\n",
      "/export/home/blangenb/miniconda3/lib/python3.6/site-packages/dask/utils.py:1010: UserWarning: Deprecated, see dask.base.get_scheduler instead\n",
      "  warnings.warn(\"Deprecated, see dask.base.get_scheduler instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n",
      "2002-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2002-01-09 00:00:00\n",
      "2002-01-17 00:00:00\n",
      "2002-01-25 00:00:00\n",
      "2002-02-02 00:00:00\n",
      "2002-02-10 00:00:00\n",
      "2002-02-18 00:00:00\n",
      "2002-02-26 00:00:00\n",
      "2002-03-06 00:00:00\n",
      "2002-03-14 00:00:00\n",
      "2002-03-22 00:00:00\n",
      "2002-03-30 00:00:00\n",
      "2002-04-07 00:00:00\n",
      "2002-04-15 00:00:00\n",
      "2002-04-23 00:00:00\n",
      "2002-05-01 00:00:00\n",
      "2002-05-09 00:00:00\n",
      "2002-05-17 00:00:00\n",
      "2002-05-25 00:00:00\n",
      "2002-06-02 00:00:00\n",
      "2002-06-10 00:00:00\n",
      "2002-06-18 00:00:00\n",
      "2002-06-26 00:00:00\n",
      "2002-07-04 00:00:00\n",
      "2002-07-12 00:00:00\n",
      "2002-07-20 00:00:00\n",
      "2002-07-28 00:00:00\n",
      "2002-08-05 00:00:00\n",
      "2002-08-13 00:00:00\n",
      "2002-08-21 00:00:00\n",
      "2002-08-29 00:00:00\n",
      "2002-09-06 00:00:00\n",
      "2002-09-14 00:00:00\n",
      "2002-09-22 00:00:00\n",
      "2002-09-30 00:00:00\n",
      "2002-10-08 00:00:00\n",
      "2002-10-16 00:00:00\n",
      "2002-10-24 00:00:00\n",
      "2002-11-01 00:00:00\n",
      "2002-11-09 00:00:00\n",
      "2002-11-17 00:00:00\n",
      "2002-11-25 00:00:00\n",
      "2002-12-03 00:00:00\n",
      "2002-12-11 00:00:00\n",
      "2002-12-19 00:00:00\n",
      "2002-12-27 00:00:00\n",
      "2003\n",
      "2003-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2003-01-09 00:00:00\n",
      "2003-01-17 00:00:00\n",
      "2003-01-25 00:00:00\n",
      "2003-02-02 00:00:00\n",
      "2003-02-10 00:00:00\n",
      "2003-02-18 00:00:00\n",
      "2003-02-26 00:00:00\n",
      "2003-03-06 00:00:00\n",
      "2003-03-14 00:00:00\n",
      "2003-03-22 00:00:00\n",
      "2003-03-30 00:00:00\n",
      "2003-04-07 00:00:00\n",
      "2003-04-15 00:00:00\n",
      "2003-04-23 00:00:00\n",
      "2003-05-01 00:00:00\n",
      "2003-05-09 00:00:00\n",
      "2003-05-17 00:00:00\n",
      "2003-05-25 00:00:00\n",
      "2003-06-02 00:00:00\n",
      "2003-06-10 00:00:00\n",
      "2003-06-18 00:00:00\n",
      "2003-06-26 00:00:00\n",
      "2003-07-04 00:00:00\n",
      "2003-07-12 00:00:00\n",
      "2003-07-20 00:00:00\n",
      "2003-07-28 00:00:00\n",
      "2003-08-05 00:00:00\n",
      "2003-08-13 00:00:00\n",
      "2003-08-21 00:00:00\n",
      "2003-08-29 00:00:00\n",
      "2003-09-06 00:00:00\n",
      "2003-09-14 00:00:00\n",
      "2003-09-22 00:00:00\n",
      "2003-09-30 00:00:00\n",
      "2003-10-08 00:00:00\n",
      "2003-10-16 00:00:00\n",
      "2003-10-24 00:00:00\n",
      "2003-11-01 00:00:00\n",
      "2003-11-09 00:00:00\n",
      "2003-11-17 00:00:00\n",
      "2003-11-25 00:00:00\n",
      "2003-12-03 00:00:00\n",
      "2003-12-11 00:00:00\n",
      "2003-12-19 00:00:00\n",
      "2003-12-27 00:00:00\n",
      "2004\n",
      "2004-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2004-01-09 00:00:00\n",
      "2004-01-17 00:00:00\n",
      "2004-01-25 00:00:00\n",
      "2004-02-02 00:00:00\n",
      "2004-02-10 00:00:00\n",
      "2004-02-18 00:00:00\n",
      "2004-02-26 00:00:00\n",
      "2004-03-05 00:00:00\n",
      "2004-03-13 00:00:00\n",
      "2004-03-21 00:00:00\n",
      "2004-03-29 00:00:00\n",
      "2004-04-06 00:00:00\n",
      "2004-04-14 00:00:00\n",
      "2004-04-22 00:00:00\n",
      "2004-04-30 00:00:00\n",
      "2004-05-08 00:00:00\n",
      "2004-05-16 00:00:00\n",
      "2004-05-24 00:00:00\n",
      "2004-06-01 00:00:00\n",
      "2004-06-09 00:00:00\n",
      "2004-06-17 00:00:00\n",
      "2004-06-25 00:00:00\n",
      "2004-07-03 00:00:00\n",
      "2004-07-11 00:00:00\n",
      "2004-07-19 00:00:00\n",
      "2004-07-27 00:00:00\n",
      "2004-08-04 00:00:00\n",
      "2004-08-12 00:00:00\n",
      "2004-08-20 00:00:00\n",
      "2004-08-28 00:00:00\n",
      "2004-09-05 00:00:00\n",
      "2004-09-13 00:00:00\n",
      "2004-09-21 00:00:00\n",
      "2004-09-29 00:00:00\n",
      "2004-10-07 00:00:00\n",
      "2004-10-15 00:00:00\n",
      "2004-10-23 00:00:00\n",
      "2004-10-31 00:00:00\n",
      "2004-11-08 00:00:00\n",
      "2004-11-16 00:00:00\n",
      "2004-11-24 00:00:00\n",
      "2004-12-02 00:00:00\n",
      "2004-12-10 00:00:00\n",
      "2004-12-18 00:00:00\n",
      "2004-12-26 00:00:00\n",
      "2005\n",
      "2005-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2005-01-09 00:00:00\n",
      "2005-01-17 00:00:00\n",
      "2005-01-25 00:00:00\n",
      "2005-02-02 00:00:00\n",
      "2005-02-10 00:00:00\n",
      "2005-02-18 00:00:00\n",
      "2005-02-26 00:00:00\n",
      "2005-03-06 00:00:00\n",
      "2005-03-14 00:00:00\n",
      "2005-03-22 00:00:00\n",
      "2005-03-30 00:00:00\n",
      "2005-04-07 00:00:00\n",
      "2005-04-15 00:00:00\n",
      "2005-04-23 00:00:00\n",
      "2005-05-01 00:00:00\n",
      "2005-05-09 00:00:00\n",
      "2005-05-17 00:00:00\n",
      "2005-05-25 00:00:00\n",
      "2005-06-02 00:00:00\n",
      "2005-06-10 00:00:00\n",
      "2005-06-18 00:00:00\n",
      "2005-06-26 00:00:00\n",
      "2005-07-04 00:00:00\n",
      "2005-07-12 00:00:00\n",
      "2005-07-20 00:00:00\n",
      "2005-07-28 00:00:00\n",
      "2005-08-05 00:00:00\n",
      "2005-08-13 00:00:00\n",
      "2005-08-21 00:00:00\n",
      "2005-08-29 00:00:00\n",
      "2005-09-06 00:00:00\n",
      "2005-09-14 00:00:00\n",
      "2005-09-22 00:00:00\n",
      "2005-09-30 00:00:00\n",
      "2005-10-08 00:00:00\n",
      "2005-10-16 00:00:00\n",
      "2005-10-24 00:00:00\n",
      "2005-11-01 00:00:00\n",
      "2005-11-09 00:00:00\n",
      "2005-11-17 00:00:00\n",
      "2005-11-25 00:00:00\n",
      "2005-12-03 00:00:00\n",
      "2005-12-11 00:00:00\n",
      "2005-12-19 00:00:00\n",
      "2005-12-27 00:00:00\n",
      "2006\n",
      "2006-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2006-01-09 00:00:00\n",
      "2006-01-17 00:00:00\n",
      "2006-01-25 00:00:00\n",
      "2006-02-02 00:00:00\n",
      "2006-02-10 00:00:00\n",
      "2006-02-18 00:00:00\n",
      "2006-02-26 00:00:00\n",
      "2006-03-06 00:00:00\n",
      "2006-03-14 00:00:00\n",
      "2006-03-22 00:00:00\n",
      "2006-03-30 00:00:00\n",
      "2006-04-07 00:00:00\n",
      "2006-04-15 00:00:00\n",
      "2006-04-23 00:00:00\n",
      "2006-05-01 00:00:00\n",
      "2006-05-09 00:00:00\n",
      "2006-05-17 00:00:00\n",
      "2006-05-25 00:00:00\n",
      "2006-06-02 00:00:00\n",
      "2006-06-10 00:00:00\n",
      "2006-06-18 00:00:00\n",
      "2006-06-26 00:00:00\n",
      "2006-07-04 00:00:00\n",
      "2006-07-12 00:00:00\n",
      "2006-07-20 00:00:00\n",
      "2006-07-28 00:00:00\n",
      "2006-08-05 00:00:00\n",
      "2006-08-13 00:00:00\n",
      "2006-08-21 00:00:00\n",
      "2006-08-29 00:00:00\n",
      "2006-09-06 00:00:00\n",
      "2006-09-14 00:00:00\n",
      "2006-09-22 00:00:00\n",
      "2006-09-30 00:00:00\n",
      "2006-10-08 00:00:00\n",
      "2006-10-16 00:00:00\n",
      "2006-10-24 00:00:00\n",
      "2006-11-01 00:00:00\n",
      "2006-11-09 00:00:00\n",
      "2006-11-17 00:00:00\n",
      "2006-11-25 00:00:00\n",
      "2006-12-03 00:00:00\n",
      "2006-12-11 00:00:00\n",
      "2006-12-19 00:00:00\n",
      "2006-12-27 00:00:00\n",
      "2007\n",
      "2007-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2007-01-09 00:00:00\n",
      "2007-01-17 00:00:00\n",
      "2007-01-25 00:00:00\n",
      "2007-02-02 00:00:00\n",
      "2007-02-10 00:00:00\n",
      "2007-02-18 00:00:00\n",
      "2007-02-26 00:00:00\n",
      "2007-03-06 00:00:00\n",
      "2007-03-14 00:00:00\n",
      "2007-03-22 00:00:00\n",
      "2007-03-30 00:00:00\n",
      "2007-04-07 00:00:00\n",
      "2007-04-15 00:00:00\n",
      "2007-04-23 00:00:00\n",
      "2007-05-01 00:00:00\n",
      "2007-05-09 00:00:00\n",
      "2007-05-17 00:00:00\n",
      "2007-05-25 00:00:00\n",
      "2007-06-02 00:00:00\n",
      "2007-06-10 00:00:00\n",
      "2007-06-18 00:00:00\n",
      "2007-06-26 00:00:00\n",
      "2007-07-04 00:00:00\n",
      "2007-07-12 00:00:00\n",
      "2007-07-20 00:00:00\n",
      "2007-07-28 00:00:00\n",
      "2007-08-05 00:00:00\n",
      "2007-08-13 00:00:00\n",
      "2007-08-21 00:00:00\n",
      "2007-08-29 00:00:00\n",
      "2007-09-06 00:00:00\n",
      "2007-09-14 00:00:00\n",
      "2007-09-22 00:00:00\n",
      "2007-09-30 00:00:00\n",
      "2007-10-08 00:00:00\n",
      "2007-10-16 00:00:00\n",
      "2007-10-24 00:00:00\n",
      "2007-11-01 00:00:00\n",
      "2007-11-09 00:00:00\n",
      "2007-11-17 00:00:00\n",
      "2007-11-25 00:00:00\n",
      "2007-12-03 00:00:00\n",
      "2007-12-11 00:00:00\n",
      "2007-12-19 00:00:00\n",
      "2007-12-27 00:00:00\n",
      "2008\n",
      "2008-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2008-01-09 00:00:00\n",
      "2008-01-17 00:00:00\n",
      "2008-01-25 00:00:00\n",
      "2008-02-02 00:00:00\n",
      "2008-02-10 00:00:00\n",
      "2008-02-18 00:00:00\n",
      "2008-02-26 00:00:00\n",
      "2008-03-05 00:00:00\n",
      "2008-03-13 00:00:00\n",
      "2008-03-21 00:00:00\n",
      "2008-03-29 00:00:00\n",
      "2008-04-06 00:00:00\n",
      "2008-04-14 00:00:00\n",
      "2008-04-22 00:00:00\n",
      "2008-04-30 00:00:00\n",
      "2008-05-08 00:00:00\n",
      "2008-05-16 00:00:00\n",
      "2008-05-24 00:00:00\n",
      "2008-06-01 00:00:00\n",
      "2008-06-09 00:00:00\n",
      "2008-06-17 00:00:00\n",
      "2008-06-25 00:00:00\n",
      "2008-07-03 00:00:00\n",
      "2008-07-11 00:00:00\n",
      "2008-07-19 00:00:00\n",
      "2008-07-27 00:00:00\n",
      "2008-08-04 00:00:00\n",
      "2008-08-12 00:00:00\n",
      "2008-08-20 00:00:00\n",
      "2008-08-28 00:00:00\n",
      "2008-09-05 00:00:00\n",
      "2008-09-13 00:00:00\n",
      "2008-09-21 00:00:00\n",
      "2008-09-29 00:00:00\n",
      "2008-10-07 00:00:00\n",
      "2008-10-15 00:00:00\n",
      "2008-10-23 00:00:00\n",
      "2008-10-31 00:00:00\n",
      "2008-11-08 00:00:00\n",
      "2008-11-16 00:00:00\n",
      "2008-11-24 00:00:00\n",
      "2008-12-02 00:00:00\n",
      "2008-12-10 00:00:00\n",
      "2008-12-18 00:00:00\n",
      "2008-12-26 00:00:00\n",
      "2009\n",
      "2009-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2009-01-09 00:00:00\n",
      "2009-01-17 00:00:00\n",
      "2009-01-25 00:00:00\n",
      "2009-02-02 00:00:00\n",
      "2009-02-10 00:00:00\n",
      "2009-02-18 00:00:00\n",
      "2009-02-26 00:00:00\n",
      "2009-03-06 00:00:00\n",
      "2009-03-14 00:00:00\n",
      "2009-03-22 00:00:00\n",
      "2009-03-30 00:00:00\n",
      "2009-04-07 00:00:00\n",
      "2009-04-15 00:00:00\n",
      "2009-04-23 00:00:00\n",
      "2009-05-01 00:00:00\n",
      "2009-05-09 00:00:00\n",
      "2009-05-17 00:00:00\n",
      "2009-05-25 00:00:00\n",
      "2009-06-02 00:00:00\n",
      "2009-06-10 00:00:00\n",
      "2009-06-18 00:00:00\n",
      "2009-06-26 00:00:00\n",
      "2009-07-04 00:00:00\n",
      "2009-07-12 00:00:00\n",
      "2009-07-20 00:00:00\n",
      "2009-07-28 00:00:00\n",
      "2009-08-05 00:00:00\n",
      "2009-08-13 00:00:00\n",
      "2009-08-21 00:00:00\n",
      "2009-08-29 00:00:00\n",
      "2009-09-06 00:00:00\n",
      "2009-09-14 00:00:00\n",
      "2009-09-22 00:00:00\n",
      "2009-09-30 00:00:00\n",
      "2009-10-08 00:00:00\n",
      "2009-10-16 00:00:00\n",
      "2009-10-24 00:00:00\n",
      "2009-11-01 00:00:00\n",
      "2009-11-09 00:00:00\n",
      "2009-11-17 00:00:00\n",
      "2009-11-25 00:00:00\n",
      "2009-12-03 00:00:00\n",
      "2009-12-11 00:00:00\n",
      "2009-12-19 00:00:00\n",
      "2009-12-27 00:00:00\n",
      "2010\n",
      "2010-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2010-01-09 00:00:00\n",
      "2010-01-17 00:00:00\n",
      "2010-01-25 00:00:00\n",
      "2010-02-02 00:00:00\n",
      "2010-02-10 00:00:00\n",
      "2010-02-18 00:00:00\n",
      "2010-02-26 00:00:00\n",
      "2010-03-06 00:00:00\n",
      "2010-03-14 00:00:00\n",
      "2010-03-22 00:00:00\n",
      "2010-03-30 00:00:00\n",
      "2010-04-07 00:00:00\n",
      "2010-04-15 00:00:00\n",
      "2010-04-23 00:00:00\n",
      "2010-05-01 00:00:00\n",
      "2010-05-09 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-05-17 00:00:00\n",
      "2010-05-25 00:00:00\n",
      "2010-06-02 00:00:00\n",
      "2010-06-10 00:00:00\n",
      "2010-06-18 00:00:00\n",
      "2010-06-26 00:00:00\n",
      "2010-07-04 00:00:00\n",
      "2010-07-12 00:00:00\n",
      "2010-07-20 00:00:00\n",
      "2010-07-28 00:00:00\n",
      "2010-08-05 00:00:00\n",
      "2010-08-13 00:00:00\n",
      "2010-08-21 00:00:00\n",
      "2010-08-29 00:00:00\n",
      "2010-09-06 00:00:00\n",
      "2010-09-14 00:00:00\n",
      "2010-09-22 00:00:00\n",
      "2010-09-30 00:00:00\n",
      "2010-10-08 00:00:00\n",
      "2010-10-16 00:00:00\n",
      "2010-10-24 00:00:00\n",
      "2010-11-01 00:00:00\n",
      "2010-11-09 00:00:00\n",
      "2010-11-17 00:00:00\n",
      "2010-11-25 00:00:00\n",
      "2010-12-03 00:00:00\n",
      "2010-12-11 00:00:00\n",
      "2010-12-19 00:00:00\n",
      "2010-12-27 00:00:00\n",
      "2011\n",
      "2011-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2011-01-09 00:00:00\n",
      "2011-01-17 00:00:00\n",
      "2011-01-25 00:00:00\n",
      "2011-02-02 00:00:00\n",
      "2011-02-10 00:00:00\n",
      "2011-02-18 00:00:00\n",
      "2011-02-26 00:00:00\n",
      "2011-03-06 00:00:00\n",
      "2011-03-14 00:00:00\n",
      "2011-03-22 00:00:00\n",
      "2011-03-30 00:00:00\n",
      "2011-04-07 00:00:00\n",
      "2011-04-15 00:00:00\n",
      "2011-04-23 00:00:00\n",
      "2011-05-01 00:00:00\n",
      "2011-05-09 00:00:00\n",
      "2011-05-17 00:00:00\n",
      "2011-05-25 00:00:00\n",
      "2011-06-02 00:00:00\n",
      "2011-06-10 00:00:00\n",
      "2011-06-18 00:00:00\n",
      "2011-06-26 00:00:00\n",
      "2011-07-04 00:00:00\n",
      "2011-07-12 00:00:00\n",
      "2011-07-20 00:00:00\n",
      "2011-07-28 00:00:00\n",
      "2011-08-05 00:00:00\n",
      "2011-08-13 00:00:00\n",
      "2011-08-21 00:00:00\n",
      "2011-08-29 00:00:00\n",
      "2011-09-06 00:00:00\n",
      "2011-09-14 00:00:00\n",
      "2011-09-22 00:00:00\n",
      "2011-09-30 00:00:00\n",
      "2011-10-08 00:00:00\n",
      "2011-10-16 00:00:00\n",
      "2011-10-24 00:00:00\n",
      "2011-11-01 00:00:00\n",
      "2011-11-09 00:00:00\n",
      "2011-11-17 00:00:00\n",
      "2011-11-25 00:00:00\n",
      "2011-12-03 00:00:00\n",
      "2011-12-11 00:00:00\n",
      "2011-12-19 00:00:00\n",
      "2011-12-27 00:00:00\n",
      "2012\n",
      "2012-01-01 00:00:00\n",
      "========== looping over 8-day intervals ==========\n",
      "2012-01-09 00:00:00\n",
      "2012-01-17 00:00:00\n",
      "2012-01-25 00:00:00\n",
      "2012-02-02 00:00:00\n",
      "2012-02-10 00:00:00\n",
      "2012-02-18 00:00:00\n",
      "2012-02-26 00:00:00\n",
      "2012-03-05 00:00:00\n",
      "2012-03-13 00:00:00\n",
      "2012-03-21 00:00:00\n",
      "2012-03-29 00:00:00\n",
      "2012-04-06 00:00:00\n",
      "2012-04-14 00:00:00\n",
      "2012-04-22 00:00:00\n",
      "2012-04-30 00:00:00\n",
      "2012-05-08 00:00:00\n",
      "2012-05-16 00:00:00\n",
      "2012-05-24 00:00:00\n",
      "2012-06-01 00:00:00\n",
      "2012-06-09 00:00:00\n",
      "2012-06-17 00:00:00\n",
      "2012-06-25 00:00:00\n",
      "2012-07-03 00:00:00\n",
      "2012-07-11 00:00:00\n",
      "2012-07-19 00:00:00\n",
      "2012-07-27 00:00:00\n",
      "2012-08-04 00:00:00\n",
      "2012-08-12 00:00:00\n",
      "2012-08-20 00:00:00\n",
      "2012-08-28 00:00:00\n",
      "2012-09-05 00:00:00\n",
      "2012-09-13 00:00:00\n",
      "2012-09-21 00:00:00\n",
      "2012-09-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "variable = 'QC_night'\n",
    "instrument = 'MOD11A2'\n",
    "\n",
    "missing_indices = []\n",
    "\n",
    "for year in range(2000,2019):\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    yearday_start = str(year)+'001'\n",
    "    yearday_end = str(year+1)+'001'\n",
    "\n",
    "#     yearday_start = str(year)+'001'\n",
    "#     yearday_end = str(year)+'020'\n",
    "    \n",
    "    yearday_start_dt = datetime.datetime.strptime(yearday_start, '%Y%j')\n",
    "    yearday_end_dt = datetime.datetime.strptime(yearday_end, '%Y%j')\n",
    "    yearday_indices = [(i<yearday_end_dt)&(i>=yearday_start_dt) for i in file_list_datetime_all]\n",
    "\n",
    "    file_list = file_list_all[yearday_indices] # list of full year files\n",
    "    file_list_datetime = file_list_datetime_all[yearday_indices]\n",
    "    file_list_yearday_strings = file_list_yearday_strings_all[yearday_indices]\n",
    "    yearday_strings_unique = numpy.unique(file_list_yearday_strings)\n",
    "\n",
    "    #create the datetime list you'll iterate over in the data set\n",
    "    year_datetime_array = numpy.array(([datetime.datetime.strptime(t, '%Y%j') for t in yearday_strings_unique]))\n",
    "\n",
    "    # pull out only the tiles of interest during the given year\n",
    "    reg_exp = re.compile('.*h(27|28|29|30|31|32)v(08|09|10)(.*\\.hdf)$')\n",
    "    file_list = numpy.array([filename for filename in file_list if re.match(reg_exp, filename)])\n",
    "\n",
    "    t = 0\n",
    "    yearday_string = yearday_strings_unique[t]\n",
    "    yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "    file_list_yearday = file_list[yearday_indices]\n",
    "    \n",
    "    if file_list_yearday.__len__()!=18:\n",
    "        print(file_list_datetime[t], 'is missing')\n",
    "        missing_indices.append(t)\n",
    "        \n",
    "    else:\n",
    "        print(year_datetime_array[t])\n",
    "        \n",
    "        # open only the files for that specific \"yearday\"\n",
    "        gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "        gdal_qc_night_data = [ds.GetSubDatasets()[5][0] for ds in gdal_datasets]\n",
    "\n",
    "        # create mosaics\n",
    "        qc_night_mosaic = gdal.BuildVRT('gdal_qc_night_mosaic.vrt', gdal_qc_night_data)\n",
    "    \n",
    "        # pull out data\n",
    "        qc_night_mosaic_data = qc_night_mosaic.ReadAsArray()\n",
    "        qc_night_mosaic_ALL_DATA = numpy.zeros((year_datetime_array.size,)+qc_night_mosaic_data.shape, dtype=numpy.int)*numpy.nan\n",
    "        qc_night_mosaic_ALL_DATA[0,:,:] = qc_night_mosaic_data\n",
    "\n",
    "    print('========== looping over 8-day intervals ==========')\n",
    "    for t in range(1,year_datetime_array.size):\n",
    "\n",
    "        yearday_string = yearday_strings_unique[t]\n",
    "        yearday_indices = [yearday_string in yd for yd in file_list]\n",
    "        file_list_yearday = file_list[yearday_indices]\n",
    "        \n",
    "        if file_list_yearday.__len__()!=18:\n",
    "            print(year_datetime_array[t], 'is missing')\n",
    "            missing_indices.append(t)\n",
    "        \n",
    "        else:\n",
    "            print(year_datetime_array[t])\n",
    "            # open only the files for that specific \"yearday\"\n",
    "            gdal_datasets = [gdal.Open(f) for f in file_list_yearday]\n",
    "            gdal_qc_night_data = [ds.GetSubDatasets()[5][0] for ds in gdal_datasets]\n",
    "\n",
    "            # create mosaics\n",
    "            qc_night_mosaic = gdal.BuildVRT('gdal_qc_night_mosaic.vrt', gdal_qc_night_data)\n",
    "            # pull out data\n",
    "            qc_night_mosaic_data = qc_night_mosaic.ReadAsArray()\n",
    "            # collect in storage arrays\n",
    "            qc_night_mosaic_ALL_DATA[t,:,:] = qc_night_mosaic_data\n",
    "\n",
    "    # set data outside valid range equal to nan\n",
    "    qc_night_mosaic_ALL_DATA[(qc_night_mosaic_ALL_DATA>255)|(qc_night_mosaic_ALL_DATA<0)] = numpy.nan\n",
    "    \n",
    "#     # scale it down to Kelvin units\n",
    "#     qc_night_mosaic_ALL_DATA_scaled = qc_night_mosaic_ALL_DATA*0.02\n",
    "#     # take the time mean\n",
    "#     qc_night_mosaic_timemean = numpy.nanmean(qc_night_mosaic_ALL_DATA_scaled, axis=0)\n",
    "    \n",
    "#     numpy.save('DATA_npy/'+instrument+'_'+variable+'_'+str(year)+'_DATA.npy', qc_night_mosaic_ALL_DATA)\n",
    "#     numpy.save('TIMES_npy/'+instrument+'_'+variable+'_'+str(year)+'_TIMES.npy', year_datetime_array)\n",
    "    \n",
    "#     dask_array = dask.array.from_array(qc_night_mosaic_ALL_DATA, chunks=(qc_night_mosaic_ALL_DATA.shape[0],100,100))\n",
    "#     dask.array.to_npy_stack('DATA_npy', dask_array)\n",
    "    \n",
    "    year_datetime_converted = netCDF4.date2num(year_datetime_array, units='days since 2000-01-01', calendar='standard')\n",
    "    qc_da = xarray.DataArray(qc_night_mosaic_ALL_DATA, dims=('time','lat','lon'))\n",
    "    qc_da['time'] = year_datetime_converted\n",
    "    qc_da['time'].attrs['units'] = 'days since 2000-01-01'\n",
    "    qc_da['time'].attrs['calendar'] = 'standard'\n",
    "    \n",
    "    qc_ds = xarray.Dataset({variable:qc_da})\n",
    "    \n",
    "    qc_ds.to_netcdf('DATA_nc/'+instrument+'_'+variable+'_'+str(year)+'.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_range = numpy.arange(0,256,1,dtype=numpy.int)\n",
    "binary_range = numpy.array([numpy.binary_repr(i, width=8) for i in int_range])\n",
    "binary_good_indices = numpy.array([i[-2:] in ['00','01'] for i in binary_range], dtype=numpy.bool)\n",
    "int_good_indices = int_range[binary_good_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   4,   5,   8,   9,  12,  13,  16,  17,  20,  21,  24,\n",
       "        25,  28,  29,  32,  33,  36,  37,  40,  41,  44,  45,  48,  49,\n",
       "        52,  53,  56,  57,  60,  61,  64,  65,  68,  69,  72,  73,  76,\n",
       "        77,  80,  81,  84,  85,  88,  89,  92,  93,  96,  97, 100, 101,\n",
       "       104, 105, 108, 109, 112, 113, 116, 117, 120, 121, 124, 125, 128,\n",
       "       129, 132, 133, 136, 137, 140, 141, 144, 145, 148, 149, 152, 153,\n",
       "       156, 157, 160, 161, 164, 165, 168, 169, 172, 173, 176, 177, 180,\n",
       "       181, 184, 185, 188, 189, 192, 193, 196, 197, 200, 201, 204, 205,\n",
       "       208, 209, 212, 213, 216, 217, 220, 221, 224, 225, 228, 229, 232,\n",
       "       233, 236, 237, 240, 241, 244, 245, 248, 249, 252, 253])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_good_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00000000', '00000001', '00000010', '00000011', '00000100',\n",
       "       '00000101', '00000110', '00000111', '00001000', '00001001',\n",
       "       '00001010', '00001011', '00001100', '00001101', '00001110',\n",
       "       '00001111', '00010000', '00010001', '00010010', '00010011',\n",
       "       '00010100', '00010101', '00010110', '00010111', '00011000',\n",
       "       '00011001', '00011010', '00011011', '00011100', '00011101',\n",
       "       '00011110', '00011111', '00100000', '00100001', '00100010',\n",
       "       '00100011', '00100100', '00100101', '00100110', '00100111',\n",
       "       '00101000', '00101001', '00101010', '00101011', '00101100',\n",
       "       '00101101', '00101110', '00101111', '00110000', '00110001',\n",
       "       '00110010', '00110011', '00110100', '00110101', '00110110',\n",
       "       '00110111', '00111000', '00111001', '00111010', '00111011',\n",
       "       '00111100', '00111101', '00111110', '00111111', '01000000',\n",
       "       '01000001', '01000010', '01000011', '01000100', '01000101',\n",
       "       '01000110', '01000111', '01001000', '01001001', '01001010',\n",
       "       '01001011', '01001100', '01001101', '01001110', '01001111',\n",
       "       '01010000', '01010001', '01010010', '01010011', '01010100',\n",
       "       '01010101', '01010110', '01010111', '01011000', '01011001',\n",
       "       '01011010', '01011011', '01011100', '01011101', '01011110',\n",
       "       '01011111', '01100000', '01100001', '01100010', '01100011',\n",
       "       '01100100', '01100101', '01100110', '01100111', '01101000',\n",
       "       '01101001', '01101010', '01101011', '01101100', '01101101',\n",
       "       '01101110', '01101111', '01110000', '01110001', '01110010',\n",
       "       '01110011', '01110100', '01110101', '01110110', '01110111',\n",
       "       '01111000', '01111001', '01111010', '01111011', '01111100',\n",
       "       '01111101', '01111110', '01111111', '10000000', '10000001',\n",
       "       '10000010', '10000011', '10000100', '10000101', '10000110',\n",
       "       '10000111', '10001000', '10001001', '10001010', '10001011',\n",
       "       '10001100', '10001101', '10001110', '10001111', '10010000',\n",
       "       '10010001', '10010010', '10010011', '10010100', '10010101',\n",
       "       '10010110', '10010111', '10011000', '10011001', '10011010',\n",
       "       '10011011', '10011100', '10011101', '10011110', '10011111',\n",
       "       '10100000', '10100001', '10100010', '10100011', '10100100',\n",
       "       '10100101', '10100110', '10100111', '10101000', '10101001',\n",
       "       '10101010', '10101011', '10101100', '10101101', '10101110',\n",
       "       '10101111', '10110000', '10110001', '10110010', '10110011',\n",
       "       '10110100', '10110101', '10110110', '10110111', '10111000',\n",
       "       '10111001', '10111010', '10111011', '10111100', '10111101',\n",
       "       '10111110', '10111111', '11000000', '11000001', '11000010',\n",
       "       '11000011', '11000100', '11000101', '11000110', '11000111',\n",
       "       '11001000', '11001001', '11001010', '11001011', '11001100',\n",
       "       '11001101', '11001110', '11001111', '11010000', '11010001',\n",
       "       '11010010', '11010011', '11010100', '11010101', '11010110',\n",
       "       '11010111', '11011000', '11011001', '11011010', '11011011',\n",
       "       '11011100', '11011101', '11011110', '11011111', '11100000',\n",
       "       '11100001', '11100010', '11100011', '11100100', '11100101',\n",
       "       '11100110', '11100111', '11101000', '11101001', '11101010',\n",
       "       '11101011', '11101100', '11101101', '11101110', '11101111',\n",
       "       '11110000', '11110001', '11110010', '11110011', '11110100',\n",
       "       '11110101', '11110110', '11110111', '11111000', '11111001',\n",
       "       '11111010', '11111011', '11111100', '11111101', '11111110',\n",
       "       '11111111'], dtype='<U8')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
